# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# selecting dataset
test_dataset: test_kenya
num_classes: 2 # all are 2, excpet bermuda = 7. 
train_percent: 0.8

#ReefCLR or Imagenet wights? If starting_weights: ImageNet
starting_weights: ImageNet

# dataset parameters
data_root: /mnt/ssd-cluster/ben/data/full_dataset/ # provs redundant, coukd remove
data_path: /mnt/ssd-cluster/ben/data/full_dataset/
json_path: /home/ben/reef-audio-representation-learning/data/dataset.json

# Hyper parameters
num_epochs: 100
batch_size: 64
learning_rate: 0.001
weight_decay: 0.001
transform: True

#wandb project
wandb_project: Fully_trained_ResNet_augmented2

# Not to change
finetune: False
image_size: [224, 224]


# environment/computational parameters
seed: 0       # random number generator seed (long integer value)
device: cuda:1
num_workers: 4


# think below is all junk
train_label_file: [/root/10_percent_train_with_unknown.csv]
#train_label_file: [/root/10_percent_train_with_unknown.csv, /root/10p_predictions_on_unlabeled_thresh_0.1_simclr.csv]
val_label_file: /root/5_percent_val_with_unknown.csv
test_label_file: /root/10_percent_test_with_unknown.csv
unlabeled_file: /root/75_percent_unlabeled_with_unknown.csv
#inference_weights: /root/ct_classifier/model_states_paws/10p_paws_200.pt
#inference_weights: /root/ct_classifier/model_states_simclr/200.pt
inference_weights: /root/ct_classifier/model_states/resnet50_10p_200epochs.pt

# Put None if using imagenet e.g starting_weights: ImageNet

